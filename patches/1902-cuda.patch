diff --git a/examples/common.cpp b/examples/common.cpp
index 94875b0..cde80fa 100644
--- a/examples/common.cpp
+++ b/examples/common.cpp
@@ -580,13 +580,13 @@ struct llama_context_params llama_context_params_from_gpt_params(const gpt_param
 std::tuple<struct llama_model *, struct llama_context *> llama_init_from_gpt_params(const gpt_params & params) {
     auto lparams = llama_context_params_from_gpt_params(params);
 
-    llama_model * model  = llama_load_model_from_file(params.model.c_str(), lparams);
+    llama_model * model  = llama_load_model_from_file(params.model.c_str(), &lparams);
     if (model == NULL) {
         fprintf(stderr, "%s: error: failed to load model '%s'\n", __func__, params.model.c_str());
         return std::make_tuple(nullptr, nullptr);
     }
 
-    llama_context * lctx = llama_new_context_with_model(model, lparams);
+    llama_context * lctx = llama_new_context_with_model(model, &lparams);
     if (lctx == NULL) {
         fprintf(stderr, "%s: error: failed to create context with model '%s'\n", __func__, params.model.c_str());
         llama_free_model(model);
diff --git a/examples/main/main.cpp b/examples/main/main.cpp
index 2248c24..db0c5ea 100644
--- a/examples/main/main.cpp
+++ b/examples/main/main.cpp
@@ -116,7 +116,7 @@ int main(int argc, char ** argv) {
     std::tie(model, ctx) = llama_init_from_gpt_params(params);
     if (params.cfg_scale > 1.f) {
         struct llama_context_params lparams = llama_context_params_from_gpt_params(params);
-        ctx_guidance = llama_new_context_with_model(model, lparams);
+        ctx_guidance = llama_new_context_with_model(model, &lparams);
     }
 
     if (model == NULL) {
diff --git a/examples/quantize-stats/quantize-stats.cpp b/examples/quantize-stats/quantize-stats.cpp
index 6aa06ec..3908fa7 100644
--- a/examples/quantize-stats/quantize-stats.cpp
+++ b/examples/quantize-stats/quantize-stats.cpp
@@ -331,14 +331,14 @@ int main(int argc, char ** argv) {
         lparams.f16_kv     = false;
         lparams.use_mlock  = false;
 
-        model = llama_load_model_from_file(params.model.c_str(), lparams);
+        model = llama_load_model_from_file(params.model.c_str(), &lparams);
 
         if (model == NULL) {
             fprintf(stderr, "%s: error: failed to load model '%s'\n", __func__, params.model.c_str());
             return 1;
         }
 
-        ctx = llama_new_context_with_model(model, lparams);
+        ctx = llama_new_context_with_model(model, &lparams);
 
         if (ctx == NULL) {
             fprintf(stderr, "%s: error: failed to create context with model '%s'\n", __func__, params.model.c_str());
diff --git a/examples/save-load-state/save-load-state.cpp b/examples/save-load-state/save-load-state.cpp
index 4c86885..386d112 100644
--- a/examples/save-load-state/save-load-state.cpp
+++ b/examples/save-load-state/save-load-state.cpp
@@ -35,11 +35,11 @@ int main(int argc, char ** argv) {
     auto last_n_tokens_data = std::vector<llama_token>(params.repeat_last_n, 0);
 
     // init
-    auto model = llama_load_model_from_file(params.model.c_str(), lparams);
+    auto model = llama_load_model_from_file(params.model.c_str(), &lparams);
     if (model == nullptr) {
         return 1;
     }
-    auto ctx = llama_new_context_with_model(model, lparams);
+    auto ctx = llama_new_context_with_model(model, &lparams);
     if (ctx == nullptr) {
         llama_free_model(model);
         return 1;
@@ -107,7 +107,7 @@ int main(int argc, char ** argv) {
     llama_free(ctx);
 
     // make new context
-    auto ctx2 = llama_new_context_with_model(model, lparams);
+    auto ctx2 = llama_new_context_with_model(model, &lparams);
 
     // Load state (rng, logits, embedding and kv_cache) from file
     {
diff --git a/examples/train-text-from-scratch/train-text-from-scratch.cpp b/examples/train-text-from-scratch/train-text-from-scratch.cpp
index afbb4a7..7b3261f 100644
--- a/examples/train-text-from-scratch/train-text-from-scratch.cpp
+++ b/examples/train-text-from-scratch/train-text-from-scratch.cpp
@@ -3045,8 +3045,8 @@ int main(int argc, char ** argv) {
     struct llama_context_params llama_params = llama_context_default_params();
     llama_params.vocab_only = true;
 
-    struct llama_model * lmodel = llama_load_model_from_file(params.fn_vocab_model, llama_params);
-    struct llama_context * lctx = llama_new_context_with_model(lmodel, llama_params);
+    struct llama_model * lmodel = llama_load_model_from_file(params.fn_vocab_model, &llama_params);
+    struct llama_context * lctx = llama_new_context_with_model(lmodel, &llama_params);
 
     struct llama_vocab vocab;
     {
diff --git a/llama.cpp b/llama.cpp
index 2d09d6c..31be517 100644
--- a/llama.cpp
+++ b/llama.cpp
@@ -2667,9 +2667,9 @@ static void llama_model_quantize_internal(const std::string & fname_inp, const s
 
 struct llama_model * llama_load_model_from_file(
                              const char * path_model,
-            struct llama_context_params   params) {
+            struct llama_context_params * p) {
     ggml_time_init();
-
+    llama_context_params params = *p;
     llama_model * model = new llama_model;
 
     ggml_type memory_type = params.f16_kv ? GGML_TYPE_F16 : GGML_TYPE_F32;
@@ -2691,7 +2691,8 @@ void llama_free_model(struct llama_model * model) {
 
 struct llama_context * llama_new_context_with_model(
                  struct llama_model * model,
-        struct llama_context_params   params) {
+        struct llama_context_params *  p) {
+    llama_context_params params = *p;
 
     if (!model) {
         return nullptr;
@@ -2812,7 +2813,7 @@ struct llama_context * llama_new_context_with_model(
 
 struct llama_context * llama_init_from_file(
                              const char * path_model,
-            struct llama_context_params   params) {
+            struct llama_context_params  * params) {
 
     struct llama_model * model = llama_load_model_from_file(path_model, params);
     if (!model) {
diff --git a/llama.h b/llama.h
index 4596b1e..e69fca5 100644
--- a/llama.h
+++ b/llama.h
@@ -166,20 +166,20 @@ extern "C" {
 
     LLAMA_API struct llama_model * llama_load_model_from_file(
                              const char * path_model,
-            struct llama_context_params   params);
+            struct llama_context_params *  params);
 
     LLAMA_API void llama_free_model(struct llama_model * model);
 
     LLAMA_API struct llama_context * llama_new_context_with_model(
                      struct llama_model * model,
-            struct llama_context_params   params);
+            struct llama_context_params *  params);
 
     // Various functions for loading a ggml llama model.
     // Allocate (almost) all memory needed for the model.
     // Return NULL on failure
     LLAMA_API DEPRECATED(struct llama_context * llama_init_from_file(
                              const char * path_model,
-            struct llama_context_params   params),
+            struct llama_context_params  * params),
             "please use llama_load_model_from_file combined with llama_new_context_with_model instead");
 
     // Frees all allocated memory
diff --git a/tests/test-tokenizer-0.cpp b/tests/test-tokenizer-0.cpp
index 87fde16..60f40cf 100644
--- a/tests/test-tokenizer-0.cpp
+++ b/tests/test-tokenizer-0.cpp
@@ -39,14 +39,14 @@ int main(int argc, char **argv) {
 
         lparams.vocab_only = true;
 
-        model = llama_load_model_from_file(fname.c_str(), lparams);
+        model = llama_load_model_from_file(fname.c_str(), &lparams);
 
         if (model == NULL) {
             fprintf(stderr, "%s: error: failed to load vocab '%s'\n", __func__, fname.c_str());
             return 1;
         }
 
-        ctx = llama_new_context_with_model(model, lparams);
+        ctx = llama_new_context_with_model(model, &lparams);
 
         if (ctx == NULL) {
             fprintf(stderr, "%s: error: failed to load vocab '%s'\n", __func__, fname.c_str());
