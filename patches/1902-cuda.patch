diff --git a/examples/common.cpp b/examples/common.cpp
index 779605f..e3ffbde 100644
--- a/examples/common.cpp
+++ b/examples/common.cpp
@@ -647,19 +647,6 @@ std::tuple<struct llama_model *, struct llama_context *> llama_init_from_gpt_par
         return std::make_tuple(nullptr, nullptr);
     }
 
-    if (!params.lora_adapter.empty()) {
-        int err = llama_model_apply_lora_from_file(model,
-                                             params.lora_adapter.c_str(),
-                                             params.lora_base.empty() ? NULL : params.lora_base.c_str(),
-                                             params.n_threads);
-        if (err != 0) {
-            fprintf(stderr, "%s: error: failed to apply lora adapter\n", __func__);
-            llama_free(lctx);
-            llama_free_model(model);
-            return std::make_tuple(nullptr, nullptr);
-        }
-    }
-
     return std::make_tuple(model, lctx);
 }
 
@@ -1035,3 +1022,66 @@ bool console_readline(console_state & con_st, std::string & line) {
     fflush(con_st.out);
     return has_more;
 }
+
+void* load_binding_model(const char *fname, int n_ctx, int n_seed, bool memory_f16, bool mlock, bool embeddings, bool mmap, bool low_vram, int n_gpu_layers, int n_batch, const char *maingpu, const char *tensorsplit, bool numa,  float rope_freq_base, float rope_freq_scale) {
+    // load the model
+    gpt_params lparams;
+    llama_model * model;
+    llama_state * state;
+    state = new llama_state;
+    llama_context * ctx;
+    lparams.n_ctx      = n_ctx;
+    lparams.seed       = n_seed;
+    lparams.memory_f16     = memory_f16;
+    lparams.embedding  = embeddings;
+    lparams.use_mlock  = mlock;
+    lparams.n_gpu_layers = n_gpu_layers;
+    lparams.use_mmap = mmap;
+    lparams.low_vram = low_vram;
+    if (rope_freq_base != 0.0f) {
+        lparams.rope_freq_base = rope_freq_base;
+    } else {
+        lparams.rope_freq_base = 1000.0f;
+    }
+
+    if (rope_freq_scale != 0.0f) {
+        lparams.rope_freq_scale = rope_freq_scale;
+    } else {
+        lparams.rope_freq_scale =  1.0f;
+    }
+
+    lparams.model = std::string(fname);
+    if (maingpu[0] != '\0') { 
+        lparams.main_gpu = std::stoi(maingpu);
+    }
+
+    if (tensorsplit[0] != '\0') { 
+        std::string arg_next = tensorsplit;
+            // split string by , and /
+            const std::regex regex{R"([,/]+)"};
+            std::sregex_token_iterator it{arg_next.begin(), arg_next.end(), regex, -1};
+            std::vector<std::string> split_arg{it, {}};
+            GGML_ASSERT(split_arg.size() <= LLAMA_MAX_DEVICES);
+
+            for (size_t i = 0; i < LLAMA_MAX_DEVICES; ++i) {
+                if (i < split_arg.size()) {
+                    lparams.tensor_split[i] = std::stof(split_arg[i]);
+                } else {
+                    lparams.tensor_split[i] = 0.0f;
+                }
+            }  
+    }
+
+    lparams.n_batch      = n_batch;
+
+    llama_backend_init(numa);
+
+    std::tie(model, ctx) = llama_init_from_gpt_params(lparams);
+    if (model == NULL) {
+        fprintf(stderr, "%s: error: unable to load model\n", __func__);
+        return nullptr;
+    }
+    state->ctx = ctx;
+    state->model= model;
+    return state;
+}
\ No newline at end of file
diff --git a/examples/common.h b/examples/common.h
index 7086606..ea824b5 100644
--- a/examples/common.h
+++ b/examples/common.h
@@ -150,3 +150,10 @@ void console_init(console_state & con_st);
 void console_cleanup(console_state & con_st);
 void console_set_color(console_state & con_st, console_color_t color);
 bool console_readline(console_state & con_st, std::string & line);
+
+struct llama_state {
+    llama_context * ctx;
+    llama_model * model;
+};
+
+void* load_binding_model(const char *fname, int n_ctx, int n_seed, bool memory_f16, bool mlock, bool embeddings, bool mmap, bool low_vram, int n_gpu_layers, int n_batch, const char *maingpu, const char *tensorsplit, bool numa, float rope_freq_base, float rope_freq_scale);
