diff --git a/examples/common.cpp b/examples/common.cpp
index bd39d92..17ff47e 100644
--- a/examples/common.cpp
+++ b/examples/common.cpp
@@ -701,18 +701,93 @@ std::tuple<struct llama_model *, struct llama_context *> llama_init_from_gpt_par
         return std::make_tuple(nullptr, nullptr);
     }
 
-    if (!params.lora_adapter.empty()) {
-        int err = llama_model_apply_lora_from_file(model,
-                                             params.lora_adapter.c_str(),
-                                             params.lora_base.empty() ? NULL : params.lora_base.c_str(),
-                                             params.n_threads);
-        if (err != 0) {
-            fprintf(stderr, "%s: error: failed to apply lora adapter\n", __func__);
-            llama_free(lctx);
-            llama_free_model(model);
-            return std::make_tuple(nullptr, nullptr);
-        }
-    }
-
     return std::make_tuple(model, lctx);
 }
+
+
+gpt_params* create_gpt_params(const std::string& fname) {
+   gpt_params* lparams = new gpt_params;
+    fprintf(stderr, "%s: loading model %s\n", __func__, fname.c_str());
+
+    // Initialize the 'model' member with the 'fname' parameter
+    lparams->model = fname;
+
+    return lparams;
+}
+
+void* load_binding_model(const char *fname, int n_ctx, int n_seed, bool memory_f16, bool mlock, bool embeddings, bool mmap, bool low_vram, int n_gpu_layers, int n_batch, const char *maingpu, const char *tensorsplit, bool numa,  float rope_freq_base, float rope_freq_scale, float rms_norm_eps,  int n_gqa) {
+    // load the model
+    gpt_params * lparams = create_gpt_params(fname);
+    llama_model * model;
+    llama_binding_state * state;
+    state = new llama_binding_state;
+    llama_context * ctx;
+    lparams->n_ctx      = n_ctx;
+    lparams->seed       = n_seed;
+    lparams->memory_f16     = memory_f16;
+    lparams->embedding  = embeddings;
+    lparams->use_mlock  = mlock;
+    lparams->n_gpu_layers = n_gpu_layers;
+    lparams->use_mmap = mmap;
+
+    // Keep sane defaults
+    if (n_gqa != 0) {
+        lparams->n_gqa = n_gqa;
+    } else {
+        lparams->n_gqa = 1;
+    }
+
+    if (rms_norm_eps != 0.0f) {
+        lparams->rms_norm_eps = rms_norm_eps;
+    } else {
+        lparams->rms_norm_eps = LLAMA_DEFAULT_RMS_EPS;
+    }
+    
+    lparams->low_vram = low_vram;
+    if (rope_freq_base != 0.0f) {
+        lparams->rope_freq_base = rope_freq_base;
+    } else {
+        lparams->rope_freq_base = 10000.0f;
+    }
+
+    if (rope_freq_scale != 0.0f) {
+        lparams->rope_freq_scale = rope_freq_scale;
+    } else {
+        lparams->rope_freq_scale =  1.0f;
+    }
+
+    lparams->model = fname;
+    if (maingpu[0] != '\0') { 
+        lparams->main_gpu = std::stoi(maingpu);
+    }
+
+    if (tensorsplit[0] != '\0') { 
+        std::string arg_next = tensorsplit;
+            // split string by , and /
+            const std::regex regex{R"([,/]+)"};
+            std::sregex_token_iterator it{arg_next.begin(), arg_next.end(), regex, -1};
+            std::vector<std::string> split_arg{it, {}};
+            GGML_ASSERT(split_arg.size() <= LLAMA_MAX_DEVICES);
+
+            for (size_t i = 0; i < LLAMA_MAX_DEVICES; ++i) {
+                if (i < split_arg.size()) {
+                    lparams->tensor_split[i] = std::stof(split_arg[i]);
+                } else {
+                    lparams->tensor_split[i] = 0.0f;
+                }
+            }  
+    }
+
+    lparams->n_batch      = n_batch;
+
+    llama_backend_init(numa);
+
+    std::tie(model, ctx) = llama_init_from_gpt_params(*lparams);
+    if (model == NULL) {
+        fprintf(stderr, "%s: error: unable to load model\n", __func__);
+        return nullptr;
+    }
+    state->ctx = ctx;
+    state->model= model;
+    return state;
+}
\ No newline at end of file
diff --git a/examples/common.h b/examples/common.h
index 375bc0a..7e7f356 100644
--- a/examples/common.h
+++ b/examples/common.h
@@ -112,3 +112,10 @@ std::vector<llama_token> llama_tokenize(struct llama_context * ctx, const std::s
 
 std::tuple<struct llama_model *, struct llama_context *> llama_init_from_gpt_params(const gpt_params & params);
 struct llama_context_params llama_context_params_from_gpt_params(const gpt_params & params);
+
+struct llama_binding_state {
+    llama_context * ctx;
+    llama_model * model;
+};
+
+void* load_binding_model(const char *fname, int n_ctx, int n_seed, bool memory_f16, bool mlock, bool embeddings, bool mmap, bool low_vram, int n_gpu_layers, int n_batch, const char *maingpu, const char *tensorsplit, bool numa, float rope_freq_base, float rope_freq_scale, float rms_norm_eps,  int n_gqa);
\ No newline at end of file
