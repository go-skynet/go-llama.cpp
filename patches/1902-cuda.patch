diff --git a/examples/common.cpp b/examples/common.cpp
index 779605f..2af5f2d 100644
--- a/examples/common.cpp
+++ b/examples/common.cpp
@@ -1035,3 +1035,54 @@ bool console_readline(console_state & con_st, std::string & line) {
     fflush(con_st.out);
     return has_more;
 }
+
+void* load_binding_model(const char *fname, int n_ctx, int n_seed, bool memory_f16, bool mlock, bool embeddings, bool mmap, bool low_vram, int n_gpu_layers, int n_batch, const char *maingpu, const char *tensorsplit, bool numa) {
+    // load the model
+    gpt_params lparams;
+    llama_model * model;
+    llama_state * state;
+    state = new llama_state;
+    llama_context * ctx;
+    lparams.n_ctx      = n_ctx;
+    lparams.seed       = n_seed;
+    lparams.memory_f16     = memory_f16;
+    lparams.embedding  = embeddings;
+    lparams.use_mlock  = mlock;
+    lparams.n_gpu_layers = n_gpu_layers;
+    lparams.use_mmap = mmap;
+    lparams.low_vram = low_vram;
+    lparams.model = std::string(fname);
+    if (maingpu[0] != '\0') { 
+        lparams.main_gpu = std::stoi(maingpu);
+    }
+
+    if (tensorsplit[0] != '\0') { 
+        std::string arg_next = tensorsplit;
+            // split string by , and /
+            const std::regex regex{R"([,/]+)"};
+            std::sregex_token_iterator it{arg_next.begin(), arg_next.end(), regex, -1};
+            std::vector<std::string> split_arg{it, {}};
+            GGML_ASSERT(split_arg.size() <= LLAMA_MAX_DEVICES);
+
+            for (size_t i = 0; i < LLAMA_MAX_DEVICES; ++i) {
+                if (i < split_arg.size()) {
+                    lparams.tensor_split[i] = std::stof(split_arg[i]);
+                } else {
+                    lparams.tensor_split[i] = 0.0f;
+                }
+            }  
+    }
+
+    lparams.n_batch      = n_batch;
+
+    llama_backend_init(numa);
+
+    std::tie(model, ctx) = llama_init_from_gpt_params(lparams);
+    if (model == NULL) {
+        fprintf(stderr, "%s: error: unable to load model\n", __func__);
+        return nullptr;
+    }
+    state->ctx = ctx;
+    state->model= model;
+    return state;
+}
\ No newline at end of file
diff --git a/examples/common.h b/examples/common.h
index 7086606..9aab561 100644
--- a/examples/common.h
+++ b/examples/common.h
@@ -150,3 +150,10 @@ void console_init(console_state & con_st);
 void console_cleanup(console_state & con_st);
 void console_set_color(console_state & con_st, console_color_t color);
 bool console_readline(console_state & con_st, std::string & line);
+
+struct llama_state {
+    llama_context * ctx;
+    llama_model * model;
+};
+
+void* load_binding_model(const char *fname, int n_ctx, int n_seed, bool memory_f16, bool mlock, bool embeddings, bool mmap, bool low_vram, int n_gpu_layers, int n_batch, const char *maingpu, const char *tensorsplit, bool numa);
