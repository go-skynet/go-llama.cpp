diff --git a/common/common.cpp b/common/common.cpp
index d7e1a57..d4db9eb 100644
--- a/common/common.cpp
+++ b/common/common.cpp
@@ -678,19 +678,6 @@ std::tuple<struct llama_model *, struct llama_context *> llama_init_from_gpt_par
         return std::make_tuple(nullptr, nullptr);
     }
 
-    if (!params.lora_adapter.empty()) {
-        int err = llama_model_apply_lora_from_file(model,
-                                             params.lora_adapter.c_str(),
-                                             params.lora_base.empty() ? NULL : params.lora_base.c_str(),
-                                             params.n_threads);
-        if (err != 0) {
-            fprintf(stderr, "%s: error: failed to apply lora adapter\n", __func__);
-            llama_free(lctx);
-            llama_free_model(model);
-            return std::make_tuple(nullptr, nullptr);
-        }
-    }
-
     if (params.ignore_eos) {
         params.logit_bias[llama_token_eos(lctx)] = -INFINITY;
     }
@@ -765,3 +752,77 @@ std::string llama_token_to_str_bpe(const struct llama_context * ctx, llama_token
     return std::string(result.data(), result.size());
 }
 
+
+gpt_params* create_gpt_params(const std::string& fname) {
+   gpt_params* lparams = new gpt_params;
+    fprintf(stderr, "%s: loading model %s\n", __func__, fname.c_str());
+
+    // Initialize the 'model' member with the 'fname' parameter
+    lparams->model = fname;
+
+    return lparams;
+}
+
+void* load_binding_model(const char *fname, int n_ctx, int n_seed, bool memory_f16, bool mlock, bool embeddings, bool mmap, bool low_vram, int n_gpu_layers, int n_batch, const char *maingpu, const char *tensorsplit, bool numa,  float rope_freq_base, float rope_freq_scale) {
+    // load the model
+    gpt_params * lparams = create_gpt_params(fname);
+    llama_model * model;
+    llama_binding_state * state;
+    state = new llama_binding_state;
+    llama_context * ctx;
+    lparams->n_ctx      = n_ctx;
+    lparams->seed       = n_seed;
+    lparams->memory_f16     = memory_f16;
+    lparams->embedding  = embeddings;
+    lparams->use_mlock  = mlock;
+    lparams->n_gpu_layers = n_gpu_layers;
+    lparams->use_mmap = mmap;
+
+    lparams->low_vram = low_vram;
+    if (rope_freq_base != 0.0f) {
+        lparams->rope_freq_base = rope_freq_base;
+    } else {
+        lparams->rope_freq_base = 10000.0f;
+    }
+
+    if (rope_freq_scale != 0.0f) {
+        lparams->rope_freq_scale = rope_freq_scale;
+    } else {
+        lparams->rope_freq_scale =  1.0f;
+    }
+
+    lparams->model = fname;
+    if (maingpu[0] != '\0') { 
+        lparams->main_gpu = std::stoi(maingpu);
+    }
+
+    if (tensorsplit[0] != '\0') { 
+        std::string arg_next = tensorsplit;
+            // split string by , and /
+            const std::regex regex{R"([,/]+)"};
+            std::sregex_token_iterator it{arg_next.begin(), arg_next.end(), regex, -1};
+            std::vector<std::string> split_arg{it, {}};
+            GGML_ASSERT(split_arg.size() <= LLAMA_MAX_DEVICES);
+
+            for (size_t i = 0; i < LLAMA_MAX_DEVICES; ++i) {
+                if (i < split_arg.size()) {
+                    lparams->tensor_split[i] = std::stof(split_arg[i]);
+                } else {
+                    lparams->tensor_split[i] = 0.0f;
+                }
+            }  
+    }
+
+    lparams->n_batch      = n_batch;
+
+    llama_backend_init(numa);
+
+    std::tie(model, ctx) = llama_init_from_gpt_params(*lparams);
+    if (model == NULL) {
+        fprintf(stderr, "%s: error: unable to load model\n", __func__);
+        return nullptr;
+    }
+    state->ctx = ctx;
+    state->model= model;
+    return state;
+}
\ No newline at end of file
diff --git a/common/common.h b/common/common.h
index c50a6ed..40c691f 100644
--- a/common/common.h
+++ b/common/common.h
@@ -128,3 +128,11 @@ std::string llama_token_to_str(
 std::string llama_token_to_str_bpe(
     const struct llama_context * ctx,
                    llama_token   token);
+
+
+struct llama_binding_state {
+    llama_context * ctx;
+    llama_model * model;
+};
+
+void* load_binding_model(const char *fname, int n_ctx, int n_seed, bool memory_f16, bool mlock, bool embeddings, bool mmap, bool low_vram, int n_gpu_layers, int n_batch, const char *maingpu, const char *tensorsplit, bool numa, float rope_freq_base, float rope_freq_scale);
